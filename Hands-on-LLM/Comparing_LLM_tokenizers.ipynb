{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Comparing the LLM Tokenizers#\n",
        "This will show us that newer toeknizers have changed their behaviourto improve model performance. And speciallized models need specialized charachgters."
      ],
      "metadata": {
        "id": "cpfpLjgU3caO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UJfxnYqL3W1u"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will allow us to see how each tokenizer deals with a number of different kinds of tokens:\n",
        "* Capitalization.\n",
        "* Languages other than English.\n",
        "* Emojis.\n",
        "* Programming code with keywords and whitespaces often used for indentation (in languages like Python for example).\n",
        "* Numbers and digits.\n",
        "* Special tokens. These are unique tokens that have a role other than representing text. They include tokens that indicate the beginning of the text, or the end of the text (which is the way the model signals to the system that it has completed this generation), or other functions as we'll see."
      ],
      "metadata": {
        "id": "wQSQoT7n4ZLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµ é¸Ÿ Ù‡Ø§Ø§Ø§Ø§Ø§Ø§ÛŒ\n",
        "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rZgTRiEA3g7_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT uncased"
      ],
      "metadata": {
        "id": "1kcarqjR64u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63YOOUBf3yio",
        "outputId": "4f1eddb1-e1d4-4c2a-a158-565b6d50f36c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98menglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84m##ization\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mÙ‡\u001b[0m \u001b[0;30;48;2;141;160;203m##Ø§\u001b[0m \u001b[0;30;48;2;231;138;195m##Ø§\u001b[0m \u001b[0;30;48;2;166;216;84m##Ø§\u001b[0m \u001b[0;30;48;2;255;217;47m##Ø§\u001b[0m \u001b[0;30;48;2;102;194;165m##Ø§\u001b[0m \u001b[0;30;48;2;252;141;98m##Ø§\u001b[0m \u001b[0;30;48;2;141;160;203m##ÛŒ\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mtoken\u001b[0m \u001b[0;30;48;2;102;194;165m##s\u001b[0m \u001b[0;30;48;2;252;141;98mfalse\u001b[0m \u001b[0;30;48;2;141;160;203mnone\u001b[0m \u001b[0;30;48;2;231;138;195meli\u001b[0m \u001b[0;30;48;2;166;216;84m##f\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m>\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195melse\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47mtwo\u001b[0m \u001b[0;30;48;2;102;194;165mtab\u001b[0m \u001b[0;30;48;2;252;141;98m##s\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47mthree\u001b[0m \u001b[0;30;48;2;102;194;165mtab\u001b[0m \u001b[0;30;48;2;252;141;98m##s\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m12\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m*\u001b[0m \u001b[0;30;48;2;231;138;195m50\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m600\u001b[0m \u001b[0;30;48;2;102;194;165m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We noticed that:\n",
        "* The newline breaks are gone, which makes the model blind to information encoded in newlines (e.g., a chat log when each turn is in a new line).\n",
        "* All the text is in lowercase.\n",
        "* The word \"capitalization\" is encoded as two subtokens: capital ##ization.\n",
        "* The ## characters are used to indicate this token is a partial token connected to the token that precedes it. This is also a method to indicate where the spaces are, as it is assumed tokens without ## in front have a space before them.\n",
        "* The emoji and Chinese characters are gone and replaced with the [UNK] special token indicating an \"unknown token.\""
      ],
      "metadata": {
        "id": "JMH5InIb6quW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT cased"
      ],
      "metadata": {
        "id": "uo3-fNMk6-q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG69wI_K32hU",
        "outputId": "ab93e9a8-e558-4c6d-94dc-5f17b5aa7b39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mCA\u001b[0m \u001b[0;30;48;2;166;216;84m##PI\u001b[0m \u001b[0;30;48;2;255;217;47m##TA\u001b[0m \u001b[0;30;48;2;102;194;165m##L\u001b[0m \u001b[0;30;48;2;252;141;98m##I\u001b[0m \u001b[0;30;48;2;141;160;203m##Z\u001b[0m \u001b[0;30;48;2;231;138;195m##AT\u001b[0m \u001b[0;30;48;2;166;216;84m##ION\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mÙ‡\u001b[0m \u001b[0;30;48;2;141;160;203m##Ø§\u001b[0m \u001b[0;30;48;2;231;138;195m##Ø§\u001b[0m \u001b[0;30;48;2;166;216;84m##Ø§\u001b[0m \u001b[0;30;48;2;255;217;47m##Ø§\u001b[0m \u001b[0;30;48;2;102;194;165m##Ø§\u001b[0m \u001b[0;30;48;2;252;141;98m##Ø§\u001b[0m \u001b[0;30;48;2;141;160;203m##ÛŒ\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mtoken\u001b[0m \u001b[0;30;48;2;102;194;165m##s\u001b[0m \u001b[0;30;48;2;252;141;98mF\u001b[0m \u001b[0;30;48;2;141;160;203m##als\u001b[0m \u001b[0;30;48;2;231;138;195m##e\u001b[0m \u001b[0;30;48;2;166;216;84mNone\u001b[0m \u001b[0;30;48;2;255;217;47mel\u001b[0m \u001b[0;30;48;2;102;194;165m##if\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m>\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47melse\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98mtwo\u001b[0m \u001b[0;30;48;2;141;160;203mta\u001b[0m \u001b[0;30;48;2;231;138;195m##bs\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98mThree\u001b[0m \u001b[0;30;48;2;141;160;203mta\u001b[0m \u001b[0;30;48;2;231;138;195m##bs\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m12\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m50\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m600\u001b[0m \u001b[0;30;48;2;141;160;203m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how \"CAPITALIZATION\" is now represented as eight tokens: CA ##PI ##TA ##L ##I ##Z ##AT ##ION.\n",
        "\n",
        "Both BERT tokenizers wrap the input within a starting [CLS] token and a closing [SEP] token. [CLS] and [SEP] are utility tokens used to wrap the input text and they serve their own purposes. [CLS] stands for classification as it's a token used at times for sentence classification. [SEP] stands for separator, as it's used to separate sentences in some applications that require passing two sentences to a model."
      ],
      "metadata": {
        "id": "XZvFfD6n7iev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT-2"
      ],
      "metadata": {
        "id": "kTP5eWze73lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw8WaQiS7FJH",
        "outputId": "465604a2-1b95-4adb-e7a9-24f734e5b47e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAP\u001b[0m \u001b[0;30;48;2;166;216;84mITAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \u001b[0;30;48;2;231;138;195mï¿½\u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47m ï¿½\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203m ï¿½\u001b[0m \u001b[0;30;48;2;231;138;195mï¿½\u001b[0m \u001b[0;30;48;2;166;216;84mØ§\u001b[0m \u001b[0;30;48;2;255;217;47mØ§\u001b[0m \u001b[0;30;48;2;102;194;165mØ§\u001b[0m \u001b[0;30;48;2;252;141;98mØ§\u001b[0m \u001b[0;30;48;2;141;160;203mØ§\u001b[0m \u001b[0;30;48;2;231;138;195mØ§\u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47mï¿½\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mt\u001b[0m \u001b[0;30;48;2;166;216;84mok\u001b[0m \u001b[0;30;48;2;255;217;47mens\u001b[0m \u001b[0;30;48;2;102;194;165m False\u001b[0m \u001b[0;30;48;2;252;141;98m None\u001b[0m \u001b[0;30;48;2;141;160;203m el\u001b[0m \u001b[0;30;48;2;231;138;195mif\u001b[0m \u001b[0;30;48;2;166;216;84m ==\u001b[0m \u001b[0;30;48;2;255;217;47m >=\u001b[0m \u001b[0;30;48;2;102;194;165m else\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m two\u001b[0m \u001b[0;30;48;2;231;138;195m tabs\u001b[0m \u001b[0;30;48;2;166;216;84m:\"\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \"\u001b[0m \u001b[0;30;48;2;231;138;195m Three\u001b[0m \u001b[0;30;48;2;166;216;84m tabs\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m \"\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m \u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \"\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The newline breaks are represented in the tokenizer.\n",
        "* Capitalization is preserved, and the word \"CAPITALIZATION\" is represented in four tokens.\n",
        "* Theé¸Ÿ characters are now represented by multiple tokens each. While we see these tokens printed as the character, they actually stand for different tokens. For example, the emoji is broken down into the tokens with token IDs 8582, 236, and 113. The tokenizer is successful in reconstructing the original character from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]), which prints out.\n",
        "* The two tabs are represented as two tokens (token number 197 in that vocabu-lary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character."
      ],
      "metadata": {
        "id": "YAxmBn5Q8jHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "token_ids = tokenizer(\"ðŸŽµ\").input_ids\n",
        "print('ðŸŽµ')\n",
        "print(token_ids)\n",
        "show_tokens(\"ðŸŽµ\", \"gpt2\")\n",
        "print('\\n')\n",
        "print('é¸Ÿ')\n",
        "token_ids = tokenizer(\"é¸Ÿ\").input_ids\n",
        "print(token_ids)\n",
        "show_tokens(\"é¸Ÿ\", \"gpt2\")\n",
        "print('\\n')\n",
        "print('Printing two tabs')\n",
        "token_ids = tokenizer(\"   \").input_ids\n",
        "print(token_ids)\n",
        "show_tokens(\"   \", \"gpt2\")\n",
        "print('\\n')\n",
        "print('Printing four spaces')\n",
        "token_ids = tokenizer(\"    \").input_ids\n",
        "print(token_ids)\n",
        "show_tokens(\"    \", \"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_A2P20r79jL",
        "outputId": "a51fd0e0-d9b3-4299-a8b1-78239c621e9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽµ\n",
            "[8582, 236, 113]\n",
            "\u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \n",
            "\n",
            "é¸Ÿ\n",
            "[165, 116, 253]\n",
            "\u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \n",
            "\n",
            "Printing two tabs\n",
            "[220, 220, 220]\n",
            "\u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \n",
            "\n",
            "Printing four spaces\n",
            "[220, 220, 220, 220]\n",
            "\u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flan-T5"
      ],
      "metadata": {
        "id": "SekbvkkNAlax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9T2HH4l-HnO",
        "outputId": "322f7d2f-ae3e-46d1-fff7-41ab6fe03348"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165mEnglish\u001b[0m \u001b[0;30;48;2;252;141;98mand\u001b[0m \u001b[0;30;48;2;141;160;203mCA\u001b[0m \u001b[0;30;48;2;231;138;195mPI\u001b[0m \u001b[0;30;48;2;166;216;84mTAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m<unk>\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84m<unk>\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165m<unk>\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mto\u001b[0m \u001b[0;30;48;2;166;216;84mken\u001b[0m \u001b[0;30;48;2;255;217;47ms\u001b[0m \u001b[0;30;48;2;102;194;165mFal\u001b[0m \u001b[0;30;48;2;252;141;98ms\u001b[0m \u001b[0;30;48;2;141;160;203me\u001b[0m \u001b[0;30;48;2;231;138;195mNone\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47me\u001b[0m \u001b[0;30;48;2;102;194;165ml\u001b[0m \u001b[0;30;48;2;252;141;98mif\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m>\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165melse\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203mtwo\u001b[0m \u001b[0;30;48;2;231;138;195mtab\u001b[0m \u001b[0;30;48;2;166;216;84ms\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203mThree\u001b[0m \u001b[0;30;48;2;231;138;195mtab\u001b[0m \u001b[0;30;48;2;166;216;84ms\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m12.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m50\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m600\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m</s>\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Flan-T5 family of models use the SentencePiece method. We notice the following:\n",
        "* No newline or whitespace tokens; this would make it challenging for the model to work with code.\n",
        "* The emoji and Chinese characters are both replaced by the <unk> token, making the model completely blind to them"
      ],
      "metadata": {
        "id": "ishz5NEzBDc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GPT4"
      ],
      "metadata": {
        "id": "UgBDBWt-GFqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The official is `tiktoken` but this the same tokenizer on the HF platform\n",
        "show_tokens(text, \"Xenova/gpt-4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpXQtV-2GDkq",
        "outputId": "18f9e176-c0ed-4ef5-a668-229ace75dd7c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \u001b[0;30;48;2;231;138;195m ï¿½\u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47mï¿½\u001b[0m \u001b[0;30;48;2;102;194;165m Ù‡\u001b[0m \u001b[0;30;48;2;252;141;98mØ§\u001b[0m \u001b[0;30;48;2;141;160;203mØ§\u001b[0m \u001b[0;30;48;2;231;138;195mØ§\u001b[0m \u001b[0;30;48;2;166;216;84mØ§\u001b[0m \u001b[0;30;48;2;255;217;47mØ§\u001b[0m \u001b[0;30;48;2;102;194;165mØ§ÛŒ\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mshow\u001b[0m \u001b[0;30;48;2;231;138;195m_tokens\u001b[0m \u001b[0;30;48;2;166;216;84m False\u001b[0m \u001b[0;30;48;2;255;217;47m None\u001b[0m \u001b[0;30;48;2;102;194;165m elif\u001b[0m \u001b[0;30;48;2;252;141;98m ==\u001b[0m \u001b[0;30;48;2;141;160;203m >=\u001b[0m \u001b[0;30;48;2;231;138;195m else\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m two\u001b[0m \u001b[0;30;48;2;102;194;165m tabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\"\u001b[0m \u001b[0;30;48;2;141;160;203m   \u001b[0m \u001b[0;30;48;2;231;138;195m \"\u001b[0m \u001b[0;30;48;2;166;216;84m Three\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m \"\u001b[0m \u001b[0;30;48;2;141;160;203m      \u001b[0m \u001b[0;30;48;2;231;138;195m \"\n",
            "\u001b[0m \u001b[0;30;48;2;166;216;84m12\u001b[0m \u001b[0;30;48;2;255;217;47m.\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m*\u001b[0m \u001b[0;30;48;2;141;160;203m50\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m600\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fill in the middle tokens. These three tokens enable the LLM to generate a completion given not only the text before it but also considering the text after it.\n",
        "* The GPT-4 tokenizer behaves similarly to its ancestor, the GPT-2 tokenizer. Some differences are:\n",
        "* It tokenizer represents the four spaces as a single token. In fact, it has a specific token for every sequence of whitespaces up to a list of 83 whitespaces.\n",
        "* The Python keyword elif has its own token in GPT-4. Both this and the previ-ous point stem from the model's focus on code in addition to natural language.\n",
        "* The GPT-4 tokenizer uses fewer tokens to represent most words. Examples here include \"CAPITALIZATION\" (two tokens versus four) and \"tokens\" (one token versus three)."
      ],
      "metadata": {
        "id": "9TV6QIK0GMVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StarCode\n",
        "Focouses on generating code"
      ],
      "metadata": {
        "id": "ycGJlIqvHKPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to request access before being able to use this tokenizer\n",
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcT0GyhDGD_S",
        "outputId": "ad869f2e-a9b3-466c-ae46-a0b1b83e230a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47mï¿½\u001b[0m \u001b[0;30;48;2;102;194;165m Ù‡\u001b[0m \u001b[0;30;48;2;252;141;98mØ§\u001b[0m \u001b[0;30;48;2;141;160;203mØ§\u001b[0m \u001b[0;30;48;2;231;138;195mØ§\u001b[0m \u001b[0;30;48;2;166;216;84mØ§\u001b[0m \u001b[0;30;48;2;255;217;47mØ§\u001b[0m \u001b[0;30;48;2;102;194;165mØ§ÛŒ\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mshow\u001b[0m \u001b[0;30;48;2;231;138;195m_\u001b[0m \u001b[0;30;48;2;166;216;84mtokens\u001b[0m \u001b[0;30;48;2;255;217;47m False\u001b[0m \u001b[0;30;48;2;102;194;165m None\u001b[0m \u001b[0;30;48;2;252;141;98m elif\u001b[0m \u001b[0;30;48;2;141;160;203m ==\u001b[0m \u001b[0;30;48;2;231;138;195m >=\u001b[0m \u001b[0;30;48;2;166;216;84m else\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m two\u001b[0m \u001b[0;30;48;2;252;141;98m tabs\u001b[0m \u001b[0;30;48;2;141;160;203m:\"\u001b[0m \u001b[0;30;48;2;231;138;195m   \u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m Three\u001b[0m \u001b[0;30;48;2;102;194;165m tabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m \"\u001b[0m \u001b[0;30;48;2;231;138;195m      \u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m1\u001b[0m \u001b[0;30;48;2;252;141;98m2\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m5\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m6\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A major difference here to everything we've seen so far is that each digit is assigned its own token (so 600 becomes 600). The hypothesis here is that this would lead to better representation of numbers and mathematics. In GPT-2, for example, the number 870 is represented as a single token. But 871 is represented as two tokens (8 and 71). You can intuitively see how that might be confusing to the model and how it represents numbers\n",
        "\n"
      ],
      "metadata": {
        "id": "A_TcwnUJHlpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* When representing code, managing the context is important. One file might make a function call to a function that is defined in a different file. So the model needs some way of being able to identify code that is in different files in the same code repository, while making a distinction between code in different repos."
      ],
      "metadata": {
        "id": "2-9nzlvBKbwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Galactica\n",
        "An LLM for science"
      ],
      "metadata": {
        "id": "ANUFsBgHKnF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"facebook/galactica-1.3b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B53fnl5QHS1z",
        "outputId": "746271eb-141c-4218-d978-b483ebcb1aa8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAP\u001b[0m \u001b[0;30;48;2;166;216;84mITAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZATION\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \u001b[0;30;48;2;231;138;195mï¿½\u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47m ï¿½\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195mÙ‡\u001b[0m \u001b[0;30;48;2;166;216;84mØ§\u001b[0m \u001b[0;30;48;2;255;217;47mØ§\u001b[0m \u001b[0;30;48;2;102;194;165mØ§\u001b[0m \u001b[0;30;48;2;252;141;98mØ§\u001b[0m \u001b[0;30;48;2;141;160;203mØ§\u001b[0m \u001b[0;30;48;2;231;138;195mØ§\u001b[0m \u001b[0;30;48;2;166;216;84mÛŒ\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165mshow\u001b[0m \u001b[0;30;48;2;252;141;98m_\u001b[0m \u001b[0;30;48;2;141;160;203mtokens\u001b[0m \u001b[0;30;48;2;231;138;195m False\u001b[0m \u001b[0;30;48;2;166;216;84m None\u001b[0m \u001b[0;30;48;2;255;217;47m elif\u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m==\u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m>\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m else\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m two\u001b[0m \u001b[0;30;48;2;141;160;203m t\u001b[0m \u001b[0;30;48;2;231;138;195mabs\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m    \u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m Three\u001b[0m \u001b[0;30;48;2;231;138;195m t\u001b[0m \u001b[0;30;48;2;166;216;84mabs\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m       \u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m1\u001b[0m \u001b[0;30;48;2;102;194;165m2\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m*\u001b[0m \u001b[0;30;48;2;166;216;84m5\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m6\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Galactica tokenizer behaves similar to StarCoder2 in that it has code in mind. It also encodes whitespaces in the same way: assigning a single token to sequences of whitespace of different lengths. It differs in that it also does that for tabs, though. So from all the tokenizers we've seen so far, it's the only one that assigns a single token to the string made up of two tabs ('\\t\\t')."
      ],
      "metadata": {
        "id": "I4GzLK0GLwyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhbUYDIpKqXS",
        "outputId": "95366518-409e-4a5f-d3f4-720cfdd533ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mEnglish\u001b[0m \u001b[0;30;48;2;231;138;195mand\u001b[0m \u001b[0;30;48;2;166;216;84mC\u001b[0m \u001b[0;30;48;2;255;217;47mAP\u001b[0m \u001b[0;30;48;2;102;194;165mIT\u001b[0m \u001b[0;30;48;2;252;141;98mAL\u001b[0m \u001b[0;30;48;2;141;160;203mIZ\u001b[0m \u001b[0;30;48;2;231;138;195mATION\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47mï¿½\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98mï¿½\u001b[0m \u001b[0;30;48;2;141;160;203mï¿½\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mï¿½\u001b[0m \u001b[0;30;48;2;255;217;47mï¿½\u001b[0m \u001b[0;30;48;2;102;194;165mï¿½\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203mÙ‡\u001b[0m \u001b[0;30;48;2;231;138;195mØ§\u001b[0m \u001b[0;30;48;2;166;216;84mØ§\u001b[0m \u001b[0;30;48;2;255;217;47mØ§\u001b[0m \u001b[0;30;48;2;102;194;165mØ§\u001b[0m \u001b[0;30;48;2;252;141;98mØ§\u001b[0m \u001b[0;30;48;2;141;160;203mØ§\u001b[0m \u001b[0;30;48;2;231;138;195mÛŒ\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_\u001b[0m \u001b[0;30;48;2;252;141;98mto\u001b[0m \u001b[0;30;48;2;141;160;203mkens\u001b[0m \u001b[0;30;48;2;231;138;195mFalse\u001b[0m \u001b[0;30;48;2;166;216;84mNone\u001b[0m \u001b[0;30;48;2;255;217;47melif\u001b[0m \u001b[0;30;48;2;102;194;165m==\u001b[0m \u001b[0;30;48;2;252;141;98m>=\u001b[0m \u001b[0;30;48;2;141;160;203melse\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84mtwo\u001b[0m \u001b[0;30;48;2;255;217;47mtabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m  \u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195mThree\u001b[0m \u001b[0;30;48;2;166;216;84mtabs\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m     \u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\n",
            "\u001b[0m \u001b[0;30;48;2;166;216;84m1\u001b[0m \u001b[0;30;48;2;255;217;47m2\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m*\u001b[0m \u001b[0;30;48;2;231;138;195m5\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m6\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "995Cl8UBL_7G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}